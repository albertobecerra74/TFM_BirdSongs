{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birdsongs - 01.- Obteniendo Datos - WebScraping Dataset\n",
    "\n",
    "\n",
    "A partir de una web recuperamos el dataset con las grabaciones que van a formar parte de nuestro estudio. El resultado de este notebook será un fichero csv con el detalle de las grabaciones con las que iniciaremos el trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Selección del dataset\n",
    "\n",
    "La obtención del dataset se realiza a través de la web [**Xeno-Canto**](https://www.xeno-canto.org/). Es una web dedicada a compartir cantos de aves de todo el mundo e invita a investigadores, aficionados y cualquier persona interesada en las aves, a escuchar, descargar y explorar su colección de cantos.\n",
    "\n",
    "\n",
    "\n",
    "![xenocanto](./resources/xenocantoweb.png)\n",
    "\n",
    "\n",
    "Las grabaciones están sujetas a distintas condiciones de uso. Para el objeto de nuestro estudio descargaremos las que no presenten limitaciones a su uso.\n",
    "\n",
    ">Recording License\n",
    ">Recordings on xeno-canto are licensed under a small number of different Creative Commons licenses. You can search for recordings that match specific license conditions using the lic tag. Possible license conditions include Attribution (BY), NonCommercial (NC), ShareAlike (SA), and NoDerivatives (ND). Conditions should be separated by a '-' character. For instance, to find recordings that are licensed under an Attribution-NonCommercial-ShareAlike license, use lic:BY-NC-SA. See the [Creative Commons website](https://creativecommons.org/licenses/) for more details about the individual licenses.\n",
    "\n",
    "\n",
    "#### Criterios de búsqueda\n",
    "\n",
    "\n",
    "No vamos a utilizar todas las grabaciones alojadas en la web, vamos a trabajar con una selección de esta. La web presenta la ventaja de realizar búsquedas avanzadas a través de [tags](https://www.xeno-canto.org/help/search)  incluidos en la url de petición, lo que permite realizar la selección de forma sencilla. Los criterios de búsqueda serán:\n",
    "\n",
    "* **Calidad**. Las grabaciones están categorizadas en base a la calidad, desde la más alta A a la más baja E. Nosotros utilizaremos aquellas correspondientes a **A y B**.\n",
    "* **Continente**. Grabaciones pertenecientes a **Europa**\n",
    "* **Licencia**.- aquellas que no tienen limitaciones a su uso **BY-NC-SA**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Webscrapping\n",
    "\n",
    "Para obtener el dataset vamos a utilizar [**webscrapping**](https://es.wikipedia.org/wiki/Web_scraping), que es una técnica utilizada mediante programas de software para extraer información de sitios web. Usualmente, estos programas simulan la navegación de un humano en la World Wide Web ya sea utilizando el protocolo HTTP manualmente, o incrustando un navegador en una aplicación. \n",
    "\n",
    "Para ello, utilizaremos:\n",
    "\n",
    "\n",
    "* [**Requests**](http://docs.python-requests.org/en/master/user/install/#install). Permite navegar y recuperar el contenido de páginas web.\n",
    "\n",
    ">Requests is an elegant and simple HTTP library for Python, built for human beings. \n",
    "\n",
    "    conda install -c anaconda requests \n",
    "\n",
    "* [**Beautifulsoup4**](https://beautiful-soup-4.readthedocs.io/en/latest/). Permite extraer información del contenido HTML, XML de una página web recuperada.\n",
    "\n",
    ">Beautiful Soup is a library for pulling data out of HTML and XML files. It provides ways of navigating, searching, and modifying parse trees.\n",
    "\n",
    "    conda install -c anaconda beautifulsoup4 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerías de uso común en el notebook\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Funciones\n",
    "\n",
    "Funciones que se encargan de recuperar la información del contenido de las páginas que vamos recuperando con las diferentes grabaciones. Con bs4 vamos tratando el HTML recuperado y obteniendo de él las distintas grabaciones que formarán parte del data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsea columnas\n",
    "\n",
    "Recupera las columnas con los datos de la grabación a partir de una fila pasada por parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# webscraping_parse_columns(columns)\n",
    "#  argumentos: \n",
    "#      column: columnas con los datos de la grabación\n",
    "#  return: \n",
    "#      ls_columns: lista con la información de la grabación\n",
    "#----------------------------------------------------------------------------\n",
    "def webscraping_parse_columns(columns):\n",
    "    # Lista temporal con la información de la grabación\n",
    "    ls_columns = []\n",
    "    \n",
    "    # Validar previamente que la grabación se corresponde con un ave y tiene\n",
    "    # un nombre común\n",
    "    if columns[1].find(\"span\", {\"class\": \"common-name\"}) == None:\n",
    "        return ls_columns\n",
    "    \n",
    "    # Parsea columnas\n",
    "    \n",
    "    # Common-name\n",
    "    common_name = columns[1].find(\"span\", {\"class\": \"common-name\"})\n",
    "    if common_name != None:\n",
    "        ls_columns.append(common_name.text.strip())\n",
    "        \n",
    "    # Scientific-name\n",
    "    scientific_name = columns[1].find(\"span\", {\"class\": \"scientific-name\"})\n",
    "    if scientific_name != None:\n",
    "        ls_columns.append(scientific_name.text.strip())\n",
    "        \n",
    "    # Length\n",
    "    ls_columns.append(columns[2].text.strip())\n",
    "    \n",
    "    # Recordist\n",
    "    ls_columns.append(columns[3].text.strip())\n",
    "    \n",
    "    # Date\n",
    "    ls_columns.append(columns[4].text.strip())\n",
    "    \n",
    "    # Country\n",
    "    ls_columns.append(columns[6].text.strip())\n",
    "    \n",
    "    # Location\n",
    "    ls_columns.append(columns[7].text.strip())\n",
    "    \n",
    "    # Type\n",
    "    ls_columns.append(columns[9].text.strip())\n",
    "    \n",
    "    # retorna lista con la grabación parseada\n",
    "    return ls_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsea calidades de audio\n",
    "\n",
    "Recupera los tipos de calidades de audio a partir del contenido HTML de una página web pasada por parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# webscraping_parse_rows(bs4_content)\n",
    "#  argumentos: \n",
    "#      bs4_content: contenido HTML de la página\n",
    "#  return: \n",
    "#      classes: diccionario con los distintos tipos de calidades de audio\n",
    "#----------------------------------------------------------------------------\n",
    "def webscraping_parse_classes(bs4_content):\n",
    "    # diccionario con las clases\n",
    "    classes = {}\n",
    "\n",
    "    # recuperar classes a partir del tag 'li'\n",
    "    recordings = bs4_content.select('li[class=\"selected\"]')\n",
    "    \n",
    "    # itera sobre las etiquetas recuperadas y va almacenando los distintos\n",
    "    # tipos de classes en un diccionario\n",
    "    for record in recordings:\n",
    "        record_id = record.get('id')\n",
    "        \n",
    "        if record_id != None:\n",
    "            ls_id = record_id.split(\"-\")\n",
    "            k_id = ls_id[1]\n",
    "            v_classification = ls_id[2]\n",
    "            classes[k_id] = v_classification\n",
    "    \n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsea filas\n",
    "\n",
    "Recupera cada una de las filas de la tabla HTML y va tratando cada una de ellas para recuperar las columnas correspondientes a las carácteristicas e identificación de la grabación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# webscraping_parse_rows(bs4_content)\n",
    "#  argumentos: \n",
    "#      bs4_content: contenido HTML de la página\n",
    "#  return: \n",
    "#      ls_rows: lista con las grabaciones\n",
    "#----------------------------------------------------------------------------\n",
    "def webscraping_parse_rows(bs4_content):\n",
    "    # recuperar las calidades del audio en el HTML\n",
    "    classes_selected =  webscraping_parse_classes(bs4_content)\n",
    "\n",
    "    # las filas con información válida tienen 11 columnas\n",
    "    COLUMNS_ITEMS = 11\n",
    "    \n",
    "    # recuperar las etiquetas de tipo fila \"tr\" en el documento HTML\n",
    "    rows = bs4_content.select('tr')\n",
    "    \n",
    "    # lista temporal con las grabaciones\n",
    "    ls_rows = []\n",
    "    \n",
    "    # recorrer la tabla HTML y recuperar los datos de la grabación por\n",
    "    # cada una de las filas tratadas\n",
    "    for row in rows:\n",
    "        # lista temporal con las columnas de la grabación\n",
    "        ls_columns = []\n",
    "        \n",
    "        # recuperar las etiquetas de columnas \"td\" \n",
    "        columns = row.select('td')\n",
    "        \n",
    "        # parsear datos si es una fila válida\n",
    "        if len(columns) == COLUMNS_ITEMS:\n",
    "            ls_columns = webscraping_parse_columns(columns)\n",
    "        \n",
    "        # añade grabación a lista\n",
    "        if len(ls_columns) > 0:\n",
    "            # recuperar ID de la grabación \n",
    "            id_= row.find(\"div\", {\"class\": \"jp-type-single\"})\n",
    "            \n",
    "            if id_ != None:\n",
    "                # añadir identificador de la grabación a la lista\n",
    "                id_number = id_.attrs['data-xc-id'].strip()\n",
    "                ls_columns.append('XC' + id_.attrs['data-xc-id'].strip())\n",
    "\n",
    "                # añadir la clase seleccionada, y si no la localiza, el tipo es por defecto 0\n",
    "                if id_number in classes_selected:\n",
    "                    ls_columns.append(classes_selected[id_number])\n",
    "                else:\n",
    "                    ls_columns.append('0')\n",
    "                \n",
    "                # añade grabación a la lista\n",
    "                ls_rows.append(ls_columns)\n",
    "\n",
    "    # retorna lista con las grabaciones\n",
    "    return ls_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsea páginas\n",
    "\n",
    "A partir del contenido HTML de una página web, recupera la tabla con la información de las grabaciones, y va recuperando cada una de ellas, hasta llegar al número de páginas pasado por parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# webscraping_parse_pages(url_path, numpages)\n",
    "#  argumentos: \n",
    "#      url_path: url de la web a scrapear\n",
    "#      numpages: número de páginas a tratar \n",
    "#  return: \n",
    "#      ls_recordings: lista con las grabaciones\n",
    "#----------------------------------------------------------------------------\n",
    "def webscraping_parse_pages(url_path, numpages):\n",
    "    print(\">>>> scraping pages...\")\n",
    "    \n",
    "    # status code ok (código de recuperación de página ok)\n",
    "    PAGE_OK = 200\n",
    "    \n",
    "    # lista donde se almacena las grabaciones\n",
    "    ls_recordings = []\n",
    "    \n",
    "    # recupera las páginas y extrae la información de ella\n",
    "    for p in range(1, numpages):\n",
    "        # mensaje por pantalla\n",
    "        if p % 10 == 0:\n",
    "            print(\">>>> pages:\", p, end='\\r', flush=True)\n",
    "        \n",
    "        # lista temporal\n",
    "        ls_temp = []\n",
    "        \n",
    "        # recuperar el contenido de la página web \"p\". Añadir\n",
    "        # el número de la página en la URL\n",
    "        page = requests.get(url_path.strip() + str(p))\n",
    "        \n",
    "        # parsear el contenido para recuperar las grabaciones\n",
    "        if page.status_code == PAGE_OK:\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            ls_temp = webscraping_parse_rows(soup)\n",
    "        else:\n",
    "            print(\"Error recuperando página\", str(p))\n",
    "        \n",
    "        # añadir grabaciones recuperadas\n",
    "        if len(ls_temp) > 0: \n",
    "            ls_recordings = ls_recordings + ls_temp      \n",
    "\n",
    "    #retorna lista con las grabaciones  \n",
    "    return ls_recordings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliza nombre científico\n",
    "\n",
    "El nombre cienfífico no viene normalizado, con el formato de \"Genero especie\"; en algunas ocasiones inclye la subespecie. Para poder utilizar luego el dataset y agrupar la información en base a la especie, la función devuelve sólo el Género más la especie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# webscraping_normalize_scientific_name(name)\n",
    "#  argumento: \n",
    "#      name:nombre científico\n",
    "#  return: \n",
    "#      name:nombre cienfífico normalizado (género + especie). \n",
    "#----------------------------------------------------------------------------\n",
    "def webscraping_normalize_scientific_name(name):\n",
    "    if len(name.split()) > 2:\n",
    "        return name.split()[0] + ' ' + name.split()[1]\n",
    "    else:\n",
    "        return name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Genera dataframe\n",
    "\n",
    "Realiza el proceso de webscrapping de la web y crea un dataframe a partir de este. El dataframe consta de la siguiente información, por cada una de las grabaciones:\n",
    "\n",
    "* **Common**: nombre común de la especie\n",
    "* **Scientific**: nombre científico de la especie\n",
    "* **Length**: duración de la grabación en formato HH:MM\n",
    "* **Recordist**: persona que realiza la grabación\n",
    "* **Country**: país donde se realiza la grabación\n",
    "* **Location**: localidad donde se realiza la grabación\n",
    "* **Type**: tipo de canto grabado (canto, llamada, vuelo, etc, etc)\n",
    "* **ID**: identificador único de la grabación\n",
    "* **Class**: calidad del audio\n",
    "* **Seconds**: duración en número de segundos\n",
    "* **Name**: nombre científico normalizado (Genero especie)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# webscraping(url_path, max_numpages, csv_file):\n",
    "# argumentos:\n",
    "#     url_path.- url de la web a webscrapear\n",
    "#     numpages.- número de páginas a tratar\n",
    "#     csv_file .- fichero de salva del dataset\n",
    "#----------------------------------------------------------------------------\n",
    "def webscraping(url_path_search, numpages, csv_file):\n",
    "    print(\">>> Web scraping...\")\n",
    "    print(\">>>\", url_path_search)\n",
    "    print(\">>>> number of pages:\", numpages)\n",
    "    \n",
    "    # Recupera páginas y genera una lista con las grabaciones\n",
    "    ls_birdsongs = []\n",
    "    ls_birdsongs = webscraping_parse_pages(url_path_search, numpages)\n",
    "    \n",
    "    # Crea un dataframe con las grabaciones\n",
    "    if len(ls_birdsongs) > 0:\n",
    "        # nombre de las columnas\n",
    "        columns_names = ['Common','Scientific', 'Length', 'Recordist', \n",
    "                         'Date', 'Country', 'Location', 'Type','ID', 'Class']\n",
    "        \n",
    "        # dataframe\n",
    "        print(\"\\n>>> creating dataframe...\")\n",
    "        df_birdsongs = pd.DataFrame.from_records(ls_birdsongs, columns=columns_names)\n",
    "        \n",
    "        # crear columna con longitud en segundos\n",
    "        df_birdsongs['Seconds'] = df_birdsongs['Length'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))\n",
    "        \n",
    "        # convertir la columna de fecha a formato fecha\n",
    "        df_birdsongs['Date'] = df_birdsongs['Date'].apply(pd.to_datetime, format='%Y-%m-%d', errors='ignore')\n",
    "        \n",
    "        # crear columna con el nombre científico normalizado (genero + especie)\n",
    "        df_birdsongs['Name'] =  df_birdsongs['Scientific'].map(webscraping_normalize_scientific_name)\n",
    "        \n",
    "        # salvar a fichero csv\n",
    "        print(\">>> saving dataframe...\", csv_file)\n",
    "        df_birdsongs.to_csv(csv_file, index= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Scraping DataSet\n",
    "\n",
    "Lanzamos el proceso de webscrapping de la página web y generamos un fichero csv con el resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterios de búqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area\n",
    "\n",
    ">The area tag allows you to search by world area. Valid values for this tag include africa, america, asia, australia, europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continente\n",
    "area = 'europe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calidad\n",
    "\n",
    ">Recording Quality\n",
    ">Recordings are rated by quality. Quality ratings range from A (highest quality) to E (lowest quality). To search for recordings that match a certain quality rating, use the q, q<, and q> tags. For example:\n",
    "\n",
    ">* q:A will return recordings with a quality rating of A.\n",
    ">* q<:C will return recordings with a quality rating of D or E.\n",
    ">* q>:C will return recordings with a quality rating of B or A.\n",
    ">Note that not all recordings are rated. Unrated recordings will not be returned for a search on quality rating. To search explicitly for unrated recordings, use the special query q:0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calidad de las grabaciones hasta \"C\" no incluida (q>:C)\n",
    "quality = 'q>%3AC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Licencia\n",
    "\n",
    ">Recording License\n",
    ">Recordings on xeno-canto are licensed under a small number of different Creative Commons licenses. You can search for recordings that match specific license conditions using the lic tag. Possible license conditions include Attribution (BY), NonCommercial (NC), ShareAlike (SA), and NoDerivatives (ND). Conditions should be separated by a '-' character. For instance, to find recordings that are licensed under an Attribution-NonCommercial-ShareAlike license, use lic:BY-NC-SA. See the Creative Commons website for more details about the individual licenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "license = \"BY-NC-SA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping!!\n",
    "\n",
    "Lanza el proceso de scrapping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> web scraping...\n",
      ">>> https://www.xeno-canto.org/explore?query=area%3Aeurope+q>%3AC+lic%3ABY-NC-SA+&pg=\n",
      ">>>> number of pages: 1527\n",
      ">>>> scraping pages...\n",
      ">>>> pages: 1520\n",
      ">>> creating dataframe...\n",
      ">>> saving dataframe... Birdsongs_europe_q>%3AC_20190107181700.csv\n",
      ">>> Proceso completado!!!\n",
      "CPU times: user 3min 25s, sys: 1 s, total: 3min 26s\n",
      "Wall time: 35min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Criterios de Búsqueda\n",
    "# URL \n",
    "url_path = 'https://www.xeno-canto.org/'  \n",
    "\n",
    "# URL con los criterios de búsqueda\n",
    "url_path_search = url_path + \\\n",
    "                \"explore?query=\" + \\\n",
    "                \"area%3A\" + area + \\\n",
    "                \"+\" + quality + \\\n",
    "                \"+lic%3A\" + license + \\\n",
    "                \"+&pg=\" \n",
    "\n",
    "# Número de páginas a tratar (revisión manual de hasta donde llega el dataset, se podría automatizar)\n",
    "numpages = 1527\n",
    "\n",
    "# Nombre del fichero csv donde salvar el dataset\n",
    "now = datetime.datetime.now()\n",
    "csv_file = 'Birdsongs' + '_' + \\\n",
    "           area + '_' + \\\n",
    "           now.strftime(\"%Y%m%d%H%M%S\") + '.csv'\n",
    "\n",
    "# Creamos el dataset\n",
    "webscraping(url_path_search, numpages, csv_file)\n",
    "\n",
    "\n",
    "print(\">>> Proceso completado!!!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
