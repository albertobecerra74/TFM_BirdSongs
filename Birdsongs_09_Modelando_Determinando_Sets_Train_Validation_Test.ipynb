{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birdsongs - 09.- Modelando - Determinando Sets Train, Validation y Test\n",
    "\n",
    "\n",
    "### Determinar los sets de datos\n",
    "\n",
    "A partir de los datos procesados, crea los directorios de Train, Validación y Test. Crea una copia de los datos sin modificar los datos originales. Cada vez que se utilice este notebook generará un juego de datos nuevo o sustituirá a uno existente. Hay que tenerlo en cuenta si no disponemos suficiente espacio para su tratamiento.\n",
    "\n",
    "![setmodel](./resources/train_test_validation.png)\n",
    "\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets\n",
    "* http://tarangshah.com/blog/2017-12-03/train-validation-and-test-sets/\n",
    "* http://mateos.io/blog/train-test-split/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinar el tamaño de cada set de datos\n",
    "\n",
    "Se establecen los ratios del tamaño de cada dataset en función de los criterios normales que utilizan otras personas en sus estudios. Si alguno de los ratios se establece a 0, significa que no queremos datos para este set y obviamos su tratamiento.\n",
    "\n",
    "* https://www.beyondthelines.net/machine-learning/how-to-split-a-dataset/\n",
    "* https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio\n",
    "* https://www.researchgate.net/post/Is_there_an_ideal_ratio_between_a_training_set_and_validation_set_Which_trade-off_would_you_suggest\n",
    "* https://www.researchgate.net/post/can_someone_recommend_what_is_the_best_percent_of_divided_the_training_data_and_testing_data_in_neural_network_7525_or_8020_or_9010\n",
    "\n",
    "\n",
    ">There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
    ">\n",
    ">If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive).\n",
    ">\n",
    ">Assuming you have enough data to do proper held-out test data (rather than cross-validation), the following is an instructive way to get a handle on variances:\n",
    ">\n",
    ">* Split your data into training and testing (80/20 is indeed a good starting point)\n",
    ">* Split the training data into training and validation (again, 80/20 is a fair split).\n",
    ">* Subsample random selections of your training data, train the classifier with this, and record the performance on the validation set\n",
    ">* Try a series of runs with different amounts of training data: randomly sample 20% of it, say, 10 times and observe performance on the validation data, then do the same with 40%, 60%, 80%. You should see both greater performance with more data, but also lower variance across the different random samples\n",
    ">* To get a handle on variance due to the size of test data, perform the same procedure in reverse. Train on all of your training data, then randomly sample a percentage of your validation data a number of times, and observe performance. You should now find that the mean performance on small samples of your validation data is roughly the same as the performance on all the validation data, but the variance is much higher with smaller numbers of test samples\"\"\n",
    "\n",
    "\n",
    "Para generar los diferentes datasets, utilizaremos los ID de las grabaciones; esto significa que si hemos dividido una grabación en ficheros de menor tamaño, todos los ficheros que tengamos de la grabación irán al mismo set al que haya sido asignado. Así evitaremos redundancia de datos.\n",
    "\n",
    "### Resultado\n",
    "\n",
    "El resultado de este notebook es la generación de una estructura de directorios del tipo:\n",
    "\n",
    "    ./modelrootpath                     \n",
    "        /modelpath\n",
    "            /train\n",
    "            /test\n",
    "            /validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.-  Librerías\n",
    "\n",
    "Carga las librerías que se van a usar en el notebook\n",
    "\n",
    "Para dividir el dataset en los sets de train, test y validación utilizamos\n",
    "[**scikit-learn**](https://scikit-learn.org/stable) \n",
    "\n",
    "> A set of python modules for machine learning and data mining\n",
    "\n",
    "      conda install scikit-learn \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerías\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.-  Carga dataset\n",
    "\n",
    "Carga el dataset en un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga dataset\n",
    "csv_path = 'Birdsongs_My_Birdsongs_Europe_20181230103204.csv'\n",
    "df_birdsongs = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.-  Filtra por tipo de canto (si procede)\n",
    "\n",
    "Crea un dataframe filtrando por un tipo de canto en concreto, procedemos a seleccionar sólo aquellas grabaciones que pertenecen al tipo de canto seleccionado.  Tipos de cantos normalizados:\n",
    "\n",
    "* call.- llamada\n",
    "* song.- canto\n",
    "* icall.- incluye llamada\n",
    "* isong.- incluye canto\n",
    "* other.- otros tipos \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra dataframe filtrando por tipo de canto\n",
    "filter_type = ''\n",
    "#filter_type = 'song'\n",
    "\n",
    "if filter_type.strip() != '':\n",
    "    df_birdsongs = df_birdsongs.loc[df_birdsongs['nType'] == filter_type]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Normaliza dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se realizan filtrados sobre el dataset es posible que existan especies que pierdan significancia o que dejen de tener datos. Para evitar problemas con estas especies, se eliminarán del dataset si el número de muestras es inferior a 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering\n",
      "            Count\n",
      "count  103.000000\n",
      "mean   168.640777\n",
      "std     52.092086\n",
      "min    101.000000\n",
      "25%    122.000000\n",
      "50%    155.000000\n",
      "75%    214.000000\n",
      "max    250.000000\n",
      "\n",
      "After filtering\n",
      "            Count\n",
      "count  103.000000\n",
      "mean   168.640777\n",
      "std     52.092086\n",
      "min    101.000000\n",
      "25%    122.000000\n",
      "50%    155.000000\n",
      "75%    214.000000\n",
      "max    250.000000\n"
     ]
    }
   ],
   "source": [
    "# dataframe de especies\n",
    "df_species = df_birdsongs.groupby('Name')['ID'].count()\n",
    "df_species = df_species.reset_index()\n",
    "df_species.columns = ['Name', 'Count'] \n",
    "\n",
    "print(\"Before filtering\")\n",
    "print(df_species.describe())\n",
    "\n",
    "# selecciona especies con más de 10 grabaciones\n",
    "min_records = 10\n",
    "df_species = df_species.loc[df_species['Count'] > min_records]\n",
    "\n",
    "print(\"\\nAfter filtering\")\n",
    "print(df_species.describe())\n",
    "\n",
    "# elimina aquellas grabaciones de las especies que hemos eliminado del dataset\n",
    "df_birdsongs.drop(df_birdsongs.loc[~df_birdsongs['Name'].isin(list(df_species['Name'].values))].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.-  Filtra por especies (permite seleccionar muestras).  \n",
    "\n",
    "Permite generar un juego de datos para unas especies concretas o para todo el dataset. Es posible que para realizar pruebas o por el volumen de datos, deseemos utilizar un número menor de especies, para entrenar el modelo. Esto será posible cambiando el parámetro de configuración.\n",
    "\n",
    "Se muestra posteriormente el porcentaje correspondiente a cada especie, para ver cómo de balanceadas están las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Count  Porcentage\n",
      "count  103.000000  103.000000\n",
      "mean   168.640777    0.970874\n",
      "std     52.092086    0.299897\n",
      "min    101.000000    0.581462\n",
      "25%    122.000000    0.702360\n",
      "50%    155.000000    0.892343\n",
      "75%    214.000000    1.232009\n",
      "max    250.000000    1.439263\n",
      "                           Name  Count  Porcentage\n",
      "86               Sitta europaea    250    1.439263\n",
      "18                 Cettia cetti    250    1.439263\n",
      "37          Emberiza citrinella    250    1.439263\n",
      "97      Troglodytes troglodytes    250    1.439263\n",
      "75       Phylloscopus trochilus    250    1.439263\n",
      "29          Cyanistes caeruleus    250    1.439263\n",
      "43            Fringilla coelebs    250    1.439263\n",
      "19              Chloris chloris    250    1.439263\n",
      "71       Phylloscopus collybita    250    1.439263\n",
      "64                  Parus major    250    1.439263\n",
      "55            Loxia curvirostra    250    1.439263\n",
      "40           Erithacus rubecula    250    1.439263\n",
      "67               Periparus ater    250    1.439263\n",
      "91           Sylvia atricapilla    250    1.439263\n",
      "99                Turdus merula    250    1.439263\n",
      "47          Garrulus glandarius    250    1.439263\n",
      "30            Dendrocopos major    250    1.439263\n",
      "81            Pyrrhula pyrrhula    245    1.410478\n",
      "33            Dryocopus martius    239    1.375936\n",
      "100           Turdus philomelos    233    1.341393\n",
      "57        Luscinia megarhynchos    231    1.329879\n",
      "84              Regulus regulus    231    1.329879\n",
      "93              Sylvia communis    230    1.324122\n",
      "24                 Corvus corax    225    1.295337\n",
      "89                  Strix aluco    218    1.255037\n",
      "60              Motacilla flava    214    1.232009\n",
      "98               Turdus iliacus    214    1.232009\n",
      "80           Prunella modularis    212    1.220495\n",
      "44                  Fulica atra    211    1.214738\n",
      "14          Carduelis carduelis    208    1.197467\n",
      "..                          ...    ...         ...\n",
      "56              Lullula arborea    126    0.725389\n",
      "70         Phylloscopus bonelli    126    0.725389\n",
      "8            Anas platyrhynchos    124    0.713874\n",
      "58             Luscinia svecica    123    0.708117\n",
      "96               Tringa totanus    121    0.696603\n",
      "49                    Grus grus    121    0.696603\n",
      "2        Acrocephalus palustris    120    0.690846\n",
      "46          Gallinula chloropus    119    0.685089\n",
      "68         Phoenicurus ochruros    118    0.679332\n",
      "52            Linaria cannabina    118    0.679332\n",
      "26                Corvus corone    116    0.667818\n",
      "48        Glaucidium passerinum    113    0.650547\n",
      "15        Carpodacus erythrinus    112    0.644790\n",
      "25                Corvus cornix    112    0.644790\n",
      "20   Chroicocephalus ridibundus    111    0.639033\n",
      "53            Locustella naevia    110    0.633276\n",
      "31          Dendrocoptes medius    110    0.633276\n",
      "23             Coloeus monedula    110    0.633276\n",
      "11                Ardea cinerea    110    0.633276\n",
      "88           Sternula albifrons    109    0.627519\n",
      "21           Cisticola juncidis    109    0.627519\n",
      "5            Actitis hypoleucos    107    0.616005\n",
      "87                Spinus spinus    106    0.610248\n",
      "62             Numenius arquata    106    0.610248\n",
      "42               Ficedula parva    106    0.610248\n",
      "73       Phylloscopus inornatus    103    0.592976\n",
      "102           Turdus viscivorus    102    0.587219\n",
      "0              Acanthis flammea    101    0.581462\n",
      "72        Phylloscopus ibericus    101    0.581462\n",
      "12                    Asio otus    101    0.581462\n",
      "\n",
      "[103 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# número de especies (si son todas, contar los registros del dataset)\n",
    "sample = df_species['Name'].count()\n",
    "random_state = 333\n",
    "\n",
    "# crea dataframe seleccionando el número de especies determinado\n",
    "df_sample = df_species.sample(n=sample, random_state=random_state)\n",
    "\n",
    "# calcula el porcentaje que representa cada especie sobre el tamaño del dataset\n",
    "df_sample['Porcentage'] = (df_sample['Count'] / df_sample['Count'].sum()) * 100\n",
    "print(df_sample.describe())\n",
    "\n",
    "# ordena el dataframe en orden descendente\n",
    "df_sample.sort_values(by='Porcentage', ascending=False, inplace=True)\n",
    "\n",
    "print(df_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtra el dataset con las especies seleccionadas\n",
    "df_birdsongs.drop(df_birdsongs.loc[~df_birdsongs['Name'].isin(list(df_sample['Name'].values))].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.- Genera los sets de Train, Validation y Test\n",
    "\n",
    "Para generar los distintos sets al azar vamos a utilizar un módulo de sklearn que nos facilita el trabajo\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    ">Split arrays or matrices into random train and test subsets\n",
    "\n",
    "Crea una matriz de \"features\" con los ID de las grabaciones y un vector de \"labels\" con los nombres de las especies a las que pertenecen y a partir de ellas se generan los distintos sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea matriz de features y vector de labels\n",
    "X = df_birdsongs['ID'].values\n",
    "y = df_birdsongs['Name'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza **train_test_split** para generar los sets. Se ejecuta en dos pasos:\n",
    "* Genera el set de train y test\n",
    "* Divide el set de train en train y validation\n",
    "\n",
    "El resultado son tres listas, cada una de ellas con los ID de las grabaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establece ratios de cada set\n",
    "train_rate = 0.65\n",
    "validation_rate = 0.15\n",
    "test_rate = 0.20\n",
    "\n",
    "# crea set de train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_rate, random_state=1, stratify=y)\n",
    "\n",
    "# divide set de train en train y validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_rate, random_state=1, stratify=y_train)\n",
    "\n",
    "# grabaciones en el set de train\n",
    "l_train = list(X_train)\n",
    "\n",
    "# grabaciones en el set de validación\n",
    "l_val = list(X_val)\n",
    "\n",
    "# grabaciones en el set de test\n",
    "l_test = list(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.- Actualiza dataset con el set asignado a cada grabación\n",
    "\n",
    "Crea una nueva columna en el dataset indicando a que set pertenece la grabación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# set_trainvalidationtest(ID)\n",
    "#  argumentos: \n",
    "#      ID: identificador de la grabación\n",
    "#  retorna: \n",
    "#      Set en el que está localizada\n",
    "#----------------------------------------------------------------------------\n",
    "def set_trainvalidationtest(ID):\n",
    "    if ID in l_train:\n",
    "        return \"train\"\n",
    "    \n",
    "    if ID in l_val:\n",
    "        return \"validation\"\n",
    "    \n",
    "    if ID in l_test:\n",
    "        return \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea columna con el set\n",
    "df_birdsongs['set'] = df_birdsongs['ID'].apply(set_trainvalidationtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.-  Crea directorios de Train, Test y Validación\n",
    "\n",
    "Crea la estructura de directorios donde vamos a copiar las grabaciones para cada uno de los sets. La estructura es del tipo \n",
    "\n",
    "    ./modelrootpath                     \n",
    "        /modelpath\n",
    "            /train\n",
    "            /test\n",
    "            /validation\n",
    "\n",
    "\n",
    "**Atención. Si el directorio /modelpath ya existe, lo primero que hace es borrarlo, ya que se entiende que se quiere regenerar de nuevo el directorio**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorio raíz donde se alojan los datos de los modelos\n",
    "modelrootpath = './model'\n",
    "\n",
    "# directorio a crear con el juego de datos\n",
    "modelpath = os.path.join(modelrootpath, 'melsxxx')\n",
    "\n",
    "# crea el directorio raíz si no existe\n",
    "if not os.path.exists(modelrootpath):\n",
    "    os.mkdir(modelrootpath)\n",
    "\n",
    "# borra el directorio del modelo si existe y lo vuelve a generar\n",
    "if os.path.exists(modelpath):\n",
    "    shutil.rmtree(modelpath)\n",
    "    \n",
    "os.mkdir(modelpath)\n",
    "\n",
    "# crea directorios de cada set\n",
    "# train \n",
    "if train_rate > 0:\n",
    "    traindir = os.path.join(modelpath, \"train\")\n",
    "    if not os.path.exists(traindir):\n",
    "        os.mkdir(traindir)\n",
    "\n",
    "# validation \n",
    "if validation_rate > 0:\n",
    "    valdir = os.path.join(modelpath, \"validation\")\n",
    "    if not os.path.exists(valdir):\n",
    "        os.mkdir(valdir)\n",
    "\n",
    "# test \n",
    "if test_rate > 0:\n",
    "    testdir = os.path.join(modelpath, \"test\")\n",
    "    if not os.path.exists(testdir):\n",
    "        os.mkdir(testdir)       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.- Copia grabaciones a cada set correspondiente\n",
    "\n",
    "El proceso itera sobre el dataset y por cada una de las grabaciones, copia del directorio origen los datos correspondiente a la grabación en el directorio destino donde haya sido asignado (train, test, validation).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# repositorio con los datos a copiar\n",
    "datapath= './image/mels'\n",
    "\n",
    "# itera sobre el dataset y va copiando los ficheros de datos (train, test, validation)\n",
    "for index, row in df_birdsongs.iterrows():\n",
    "    # grabación\n",
    "    ID = row['ID']\n",
    "    specie = row['Name']\n",
    "    set_folder = row['set']\n",
    "\n",
    "    # selecciona todos los ficheros pertenecientes a la grabación\n",
    "    datadir = os.path.join(datapath, specie)\n",
    "    files = os.path.join(datadir, ID + '*')\n",
    "\n",
    "    # crea directorio si no existe\n",
    "    dest_dir = os.path.join(modelpath, set_folder, specie)\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.mkdir(dest_dir)  \n",
    "    \n",
    "    # copia ficheros\n",
    "    for file in glob.glob(files):\n",
    "        print(file, 'to', dest_dir, end='\\r', flush=True)\n",
    "        shutil.copy(file, dest_dir)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
